{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms.functional as FT\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_imgs =\"/data/ssd_ilija_data/original_images/*\"\n",
    "path_bboxes_labels = \"/data/ssd_ilija_data/ground_truth/bboxes_labels/\"\n",
    "imgs = glob.glob(path_imgs)\n",
    "bboxes = glob.glob(path_bboxes_labels+\"bboxes*\")\n",
    "labels = glob.glob(path_bboxes_labels+\"labels*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.sort()\n",
    "bboxes.sort()\n",
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(box_a, box_b):\n",
    "    \"\"\" Compute intersection between each two boxes in the sets box_a and box_b\n",
    "    Args:\n",
    "      :box_a  bounding boxes, Shape: (N,4).\n",
    "      :box_b  bounding boxes, Shape: (M,4).\n",
    "    Return:\n",
    "      Intersection area, Shape: (N,M).\n",
    "    \"\"\"\n",
    "\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1), box_b[:, 2:].unsqueeze(0))\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1), box_b[:, :2].unsqueeze(0))\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "def jaccard_overlap(set_a, set_b):\n",
    "\n",
    "    \"\"\"Compute Jaccard Overlap between 2 boxes, which is: a ∩ B / a ∪ b\n",
    "        :set_a set of bounding boxes Shape: (N, format of encoding)\n",
    "        :set_b: set of bounding boxes Shape: (M, format of encoding)\n",
    "        Format of encoding must be the same for both sets\n",
    "    Return:\n",
    "        a ∩ B / a ∪ b\n",
    "\n",
    "    \"\"\"\n",
    "    intersection_seta_setb = intersection(set_a, set_b)\n",
    "\n",
    "    area_set_a = ((set_a[:, 2] - set_a[:, 0]) * (set_a[:, 3] - set_a[:, 1])).unsqueeze(1)\n",
    "    area_set_b = ((set_b[:, 2] - set_b[:, 0]) * (set_b[:, 3] - set_b[:, 1])).unsqueeze(0)\n",
    "\n",
    "    union_seta_setb = area_set_a + area_set_b - intersection_seta_setb\n",
    "\n",
    "    return intersection_seta_setb/union_seta_setb\n",
    "\n",
    "def random_crop(image, boxes, labels):\n",
    "    \"\"\"\n",
    "    Performs a random crop operation with multiple possibilities as in the paper.\n",
    "    Helpful when detecting bigger objects(windows, doors, buildings). \n",
    "    Ref: https://github.com/amdegroot/ssd.pytorch/blob/master/utils/augmentations.py\n",
    "    \"\"\"\n",
    "    height, width = image.size(1), image.size(2)\n",
    "\n",
    "    while True:\n",
    "        #randomly choose a min overlap\n",
    "        mode = random.choice([0., .1, .3, .5, .7, .9, None])  # 'None' refers to no cropping\n",
    "\n",
    "        # Do not crop if NONE\n",
    "        if mode is None:\n",
    "            return image, boxes, labels\n",
    "\n",
    "        # Do 50 trials\n",
    "        max_trials = 50\n",
    "        for _ in range(max_trials):\n",
    "            \n",
    "            # Crop dimensions must be in [0.3, 1] of original dimensions\n",
    "            new_height = int(random.uniform(0.3, 1) * height)\n",
    "            new_width = int(random.uniform(0.3, 1) * width)\n",
    "\n",
    "            # Aspect ratio must be in [0.5, 2]\n",
    "            aspect_ratio = new_height / new_width\n",
    "            if not 0.5 < aspect_ratio < 2:\n",
    "                continue\n",
    "\n",
    "            # Get crop coordinates\n",
    "            left = random.randint(0, width - new_width)\n",
    "            right = left + new_width\n",
    "            top = random.randint(0, height - new_height)\n",
    "            bottom = top + new_height\n",
    "            crop = torch.FloatTensor([left, top, right, bottom])  # (4)\n",
    "\n",
    "            # Compute jaccard overlap between crop and bboxes\n",
    "            overlap = jaccard_overlap(crop.unsqueeze(0),\n",
    "                                           boxes)\n",
    "            \n",
    "            overlap = overlap.squeeze(0)  \n",
    "\n",
    "            # If all overlaps are smaller try again \n",
    "            if overlap.max().item() < mode:\n",
    "                continue\n",
    "\n",
    "            #Crop the image\n",
    "            new_image = image[:, top:bottom, left:right]\n",
    "            \n",
    "            #Get centers of bounding boxxes\n",
    "            bb_centers = (boxes[:, :2] + boxes[:, 2:]) / 2.\n",
    "\n",
    "            # Find bounding boxes whose centers are in the crop\n",
    "            centers_in_crop = (bb_centers[:, 0] > left) * (bb_centers[:, 0] < right) * (bb_centers[:, 1] > top) * (bb_centers[:, 1] < bottom)  \n",
    "\n",
    "            #If no boxes are in the crop try again\n",
    "            if not centers_in_crop.any():\n",
    "                continue\n",
    "\n",
    "            #Remove bounding boxes that do not satisfy cond\n",
    "            new_boxes = boxes[centers_in_crop, :]\n",
    "            new_labels = labels[centers_in_crop]\n",
    "\n",
    "            # Compute the positions of bounding boxes in the new img\n",
    "            new_boxes[:, :2] = torch.max(new_boxes[:, :2], crop[:2])\n",
    "            new_boxes[:, :2] -= crop[:2]\n",
    "            new_boxes[:, 2:] = torch.min(new_boxes[:, 2:], crop[2:]) \n",
    "            new_boxes[:, 2:] -= crop[:2]\n",
    "\n",
    "            return new_image, new_boxes, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(imgs[10])\n",
    "boxes = np.load(bboxes[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_boxes = torch.FloatTensor(boxes[:, :4])\n",
    "labels = torch.FloatTensor(boxes[:, 4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-df59fa176ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-a64ce0b4ff63>\u001b[0m in \u001b[0;36mrandom_crop\u001b[0;34m(image, boxes, labels)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Compute jaccard overlap between crop and bboxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             overlap = jaccard_overlap(crop.unsqueeze(0),\n\u001b[0;32m---> 73\u001b[0;31m                                            boxes)\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0moverlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-a64ce0b4ff63>\u001b[0m in \u001b[0;36mjaccard_overlap\u001b[0;34m(set_a, set_b)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mintersection_seta_setb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0marea_set_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-a64ce0b4ff63>\u001b[0m in \u001b[0;36mintersection\u001b[0;34m(box_a, box_b)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmax_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmin_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_xy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_xy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "new_img, new_bboxes, new_labels = random_crop(FT.to_tensor(img), b_boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
